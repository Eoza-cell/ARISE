const fs = require('fs').promises;
const path = require('path');

class HuggingFaceClient {
    constructor() {
        // Cl√© API Hugging Face int√©gr√©e directement pour d√©ploiement
        this.apiKey = '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';
        this.isAvailable = false;
        this.client = null;

        this.initializeClient();
    }

    async initializeClient() {
        try {
            if (!this.apiKey) {
                console.log('‚ö†Ô∏è HF_TOKEN non trouv√©e - g√©n√©ration de vid√©os Hugging Face d√©sactiv√©e');
                return;
            }

            // Initialiser le client avec la nouvelle API
            const { InferenceClient } = await import('@huggingface/inference');
            this.client = new InferenceClient(this.apiKey);

            this.isAvailable = true;
            console.log('‚úÖ Client Hugging Face initialis√© avec succ√®s (@huggingface/inference)');

        } catch (error) {
            console.error('‚ùå Erreur lors de l\'initialisation du client Hugging Face:', error.message);
            this.isAvailable = false;
        }
    }

    hasValidClient() {
        return this.isAvailable && this.apiKey && this.client;
    }

    async generateVideoFromText(prompt, outputPath, options = {}) {
        try {
            if (!this.hasValidClient()) {
                console.log('‚ö†Ô∏è Client HuggingFace non disponible - HF_TOKEN manquante');
                return null;
            }

            console.log(`ü§ó G√©n√©ration vid√©o Hugging Face avec prompt: "${prompt.substring(0, 100)}..."`);

            // Utiliser l'image du personnage si disponible
            let imageData = null;
            if (options.characterImagePath) {
                try {
                    imageData = await fs.readFile(options.characterImagePath);
                    console.log(`üì∏ Image du personnage charg√©e: ${options.characterImagePath}`);
                } catch (error) {
                    console.log(`‚ö†Ô∏è Impossible de charger l'image: ${options.characterImagePath}`);
                }
            }

            const optimizedPrompt = this.optimizePromptForImageToVideo(prompt);

            try {
                let videoBlob;

                if (imageData) {
                    // Mode image-to-video avec la nouvelle API
                    console.log(`üé¨ Mode image-to-video avec Lightricks/LTX-Video...`);

                    videoBlob = await this.client.imageToVideo({
                        provider: "fal-ai",
                        model: "Lightricks/LTX-Video",
                        inputs: imageData,
                        parameters: { 
                            prompt: optimizedPrompt,
                            duration: Math.min(options.duration || 3, 5),
                            fps: Math.min(options.fps || 8, 12)
                        }
                    });
                } else {
                    // Mode text-to-video de fallback
                    console.log(`üé¨ Mode text-to-video avec mod√®le de fallback...`);

                    videoBlob = await this.client.textToVideo({
                        model: "damo-vilab/text-to-video-ms-1.7b",
                        inputs: optimizedPrompt,
                        parameters: {
                            num_frames: options.num_frames || 16,
                            fps: options.fps || 8,
                            width: options.width || 256,
                            height: options.height || 256
                        }
                    });
                }

                // Convertir le Blob en Buffer
                const videoBuffer = Buffer.from(await videoBlob.arrayBuffer());

                // V√©rifier que c'est bien une vid√©o
                if (!videoBuffer || videoBuffer.length < 1000) {
                    throw new Error('Vid√©o g√©n√©r√©e trop petite ou invalide');
                }

                // Cr√©er le dossier et sauvegarder
                const outputDir = path.dirname(outputPath);
                await fs.mkdir(outputDir, { recursive: true });
                await fs.writeFile(outputPath, videoBuffer);

                console.log(`‚úÖ Vid√©o HuggingFace g√©n√©r√©e: ${outputPath} (${videoBuffer.length} bytes)`);

                return {
                    success: true,
                    videoPath: outputPath,
                    method: 'huggingface',
                    size: videoBuffer.length
                };

            } catch (apiError) {
                console.log(`‚ö†Ô∏è Erreur API principale: ${apiError.message}`);

                // Fallback vers l'ancienne m√©thode
                return await this.generateVideoWithLegacyFallback(prompt, outputPath, options);
            }

        } catch (error) {
            console.error('‚ùå Erreur g√©n√©ration vid√©o HuggingFace:', error.message);
            return null;
        }
    }

    async generateVideoWithLegacyFallback(prompt, outputPath, options = {}) {
        try {
            console.log(`üîÑ Fallback vers l'ancienne API...`);

            const response = await fetch('https://api-inference.huggingface.co/models/ali-vilab/text-to-video-ms-1.7b', {
                headers: {
                    'Authorization': `Bearer ${this.apiKey}`,
                    'Content-Type': 'application/json',
                },
                method: 'POST',
                body: JSON.stringify({
                    inputs: this.optimizePromptForImageToVideo(prompt),
                    parameters: {
                        num_inference_steps: 20,
                        guidance_scale: 7.5,
                        width: 256,
                        height: 256,
                        num_frames: 8
                    }
                }),
                timeout: 45000
            });

            if (!response.ok) {
                throw new Error(`Erreur mod√®le fallback: ${response.status}`);
            }

            const videoBuffer = Buffer.from(await response.arrayBuffer());

            if (videoBuffer.length < 500) {
                throw new Error('Vid√©o fallback trop petite');
            }

            const outputDir = path.dirname(outputPath);
            await fs.mkdir(outputDir, { recursive: true });
            await fs.writeFile(outputPath, videoBuffer);

            console.log(`‚úÖ Vid√©o legacy fallback g√©n√©r√©e: ${outputPath} (${videoBuffer.length} bytes)`);
            return outputPath;

        } catch (error) {
            console.log('‚ùå √âchec du fallback legacy:', error.message);
            return null;
        }
    }

    optimizePromptForImageToVideo(prompt) {
        let optimized = prompt
            .replace(/['"]/g, '')
            .replace(/\s+/g, ' ')
            .trim();

        const improvements = [
            'smooth movement',
            'natural motion',
            'fluid animation',
            'realistic movement',
            'cinematic quality'
        ];

        improvements.forEach(improvement => {
            if (!optimized.toLowerCase().includes(improvement.toLowerCase())) {
                optimized += `, ${improvement}`;
            }
        });

        if (optimized.length > 200) {
            optimized = optimized.substring(0, 197) + '...';
        }

        console.log(`üéØ Prompt optimis√© pour image-to-video: "${optimized}"`);
        return optimized;
    }

    // M√©thodes RPG sp√©cifiques avec support d'image
    async generateCombatVideo(action, character, outputPath) {
        const prompt = `${character.name} in medieval fantasy combat, ${action}, epic battle scene, dynamic movement, armor and weapons, dramatic lighting, action sequence`;

        const result = await this.generateVideoFromText(prompt, outputPath, {
            duration: 4,
            characterImagePath: character.imagePath || null
        });
        
        return result || outputPath;
    }

    async generateCharacterActionVideo(action, character, location, outputPath) {
        const prompt = `${character.name} performing ${action} in ${location}, medieval fantasy setting, character in motion, atmospheric environment, cinematic camera angle`;

        return await this.generateVideoFromText(prompt, outputPath, {
            duration: 5,
            characterImagePath: character.imagePath || null
        });
    }

    async generateLocationVideo(location, character, outputPath) {
        const prompt = `Fantasy location: ${location}, ${character.name} exploring the area, epic landscape, atmospheric lighting, cinematic camera movement, medieval fantasy world`;

        return await this.generateVideoFromText(prompt, outputPath, {
            duration: 6,
            characterImagePath: character.imagePath || null
        });
    }

    async generateMagicSpellVideo(spellName, character, outputPath) {
        const prompt = `${character.name} casting ${spellName} magic spell, mystical energy effects, glowing magical aura, fantasy spellcasting, dynamic magical particles, epic scene`;

        return await this.generateVideoFromText(prompt, outputPath, {
            duration: 4,
            characterImagePath: character.imagePath || null
        });
    }

    async generateVideoFromImage(imagePath, prompt, outputPath) {
        try {
            if (!this.hasValidClient()) {
                throw new Error('HuggingFace client non disponible - v√©rifiez HF_TOKEN');
            }

            console.log(`üé¨ G√©n√©ration vid√©o HuggingFace depuis image: ${imagePath}`);
            console.log(`üéØ Prompt: "${prompt}"`);

            // Lire l'image
            const imageBuffer = await fs.readFile(imagePath);

            // Utiliser le mod√®le LTX-Video comme configur√©
            const video = await this.client.imageToVideo({
                provider: "fal-ai",
                model: "Lightricks/LTX-Video",
                inputs: imageBuffer,
                parameters: { 
                    prompt: prompt,
                    num_frames: 121,
                    height: 704,
                    width: 1216,
                    fps: 25,
                    seed: Math.floor(Math.random() * 1000000)
                }
            });

            // Sauvegarder la vid√©o
            let videoBuffer;
            if (video instanceof Blob) {
                videoBuffer = Buffer.from(await video.arrayBuffer());
            } else if (Buffer.isBuffer(video)) {
                videoBuffer = video;
            } else {
                throw new Error('Format de vid√©o non support√©');
            }

            await fs.writeFile(outputPath, videoBuffer);

            console.log(`‚úÖ Vid√©o HuggingFace g√©n√©r√©e: ${outputPath} (${videoBuffer.length} bytes)`);
            return outputPath;

        } catch (error) {
            console.error('‚ùå Erreur g√©n√©ration vid√©o HuggingFace:', error);

            // Fallback avec un mod√®le alternatif
            try {
                console.log('üîÑ Tentative avec mod√®le alternatif...');
                const video = await this.client.imageToVideo({
                    model: "stabilityai/stable-video-diffusion-img2vid-xt",
                    inputs: imageBuffer,
                    parameters: {
                        height: 576,
                        width: 1024,
                        num_frames: 25,
                        motion_bucket_id: 127,
                        fps: 6
                    }
                });

                const videoBuffer = Buffer.from(await video.arrayBuffer());
                await fs.writeFile(outputPath, videoBuffer);

                console.log(`‚úÖ Vid√©o HuggingFace g√©n√©r√©e (fallback): ${outputPath}`);
                return outputPath;
            } catch (fallbackError) {
                console.error('‚ùå √âchec fallback:', fallbackError);
                throw error;
            }
        }
    }
}

module.exports = HuggingFaceClient;